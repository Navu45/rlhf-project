# RLHF-Project

## Обзор

Этот проект реализует метод Weight Averaged Rewarded Policies (WARP) для обучения с подкреплением на основе обратной связи от человека (RLHF), специально решая проблему больших требований к ресурсам, связанных с методом WARP, путем использования адаптеров Low-Rank Adaptation (LoRA). Этот подход позволяет эффективно усреднять веса, используя адаптеры LoRA вместо базовых слоев.

## Содержание

- [Введение](#введение)
- [Методология](#методология)
- [Установка](#установка)
- [Использование](#использование)

## Введение

Обучение с подкреплением на основе обратной связи от человека (RLHF) позволяет согласовать большие языковые модели (LLM) путем поощрения их генераций с высокими оценками на основе человеческих предпочтений. Традиционные методы RLHF включают регуляризацию Кульбака-Лейблера (KL), чтобы сбалансировать предварительно обученные знания и оптимизацию награды. Метод WARP улучшает этот компромисс, объединяя политики на разных этапах с использованием усреднения весов.

Однако, требования к вычислительным ресурсам у метода WARP значительны. Этот проект предлагает решение, используя адаптеры LoRA для эффективного усреднения весов, тем самым уменьшая потребность в ресурсах без ущерба для производительности.

## Методология

### WARP с адаптерами LoRA

Метод WARP включает три этапа объединения моделей:
1. **Экспоненциальное скользящее среднее (EMA) якоря**: Во время дообучения RL политика с EMA служит динамическим якорем для регуляризации KL.
2. **Сферическая линейная интерполяция (SLERP)**: Независимо дообученные политики объединяются с помощью SLERP, комбинируя их сильные стороны для создания улучшенной модели.
3. **Линейная интерполяция к инициализации (LITI)**: Объединенная модель линейно интерполируется с инициализацией для восстановления предварительно обученных характеристик.

Используя адаптеры LoRA, этот проект уменьшает объем памяти и вычислительную нагрузку, обычно ассоциирующуюся с этими этапами.

### Детали реализации

- **Адаптеры LoRA**: Эти адаптеры используются для эффективного усреднения весов, уменьшая размер и сложность моделей.
- **Цикл обучения**: Цикл обучения итеративно применяет методологию WARP, постепенно улучшая модель.

## Установка

Чтобы установить необходимые зависимости, выполните:

```bash
pip install -r requirements.txt
```

## Использование

Для обучения `reward`-модели выполните следующую команду:

```bash
bash reward/train_reward_model.sh
```

Для обучения модели с использованием метода WARP с адаптерами LoRA выполните следующую команду:


```bash
bash warp/train_warp_model.sh
```

### Конфигурация

Измените файлы `.sh`, чтобы настроить параметры обучения, включая набор данных, скорость обучения и другие гиперпараметры.